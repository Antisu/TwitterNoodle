import os
import bz2
import pickle 

from packages.dataset_tools import common

""" This module contains some functions which can
    be used to scale a dataset generated by
    packages.dataset_tools.generate_dataset.

    Pickled files (can be compressed as well)
    can be merged into one big file.

    One big pickled file (can also be compressed)
    can be split into smaller files.
"""


def get_filenames_in_dir(path:str) -> list:
    " Gets all filenames in the input dir"
    file_names = [item[2] for item in os.walk(path)]
    undesirables = [".DS_Store"] # What to remove from file_names
    for x in undesirables:
        try:
            file_names[0].remove(x)
        except:
            pass  
    return file_names[0]


def get_file_content(filename:str) -> list:
    """ Attempts to load a compressed (bz2)
        pickle file first. If that fails;
        attempts to load only pickle. Second
        attempt is allowed to fail if;
            - File not found.
            - Pickle load issue.
    """ 
    try: # // If zipped and pickled
        unzipped = bz2.BZ2File(filename).read()
        non_binary = pickle.loads(unzipped)
        return non_binary
    except: # // If only pickled
        # Allow failure if file not found, or pickle issue.
        with open(filename, "rb") as f:
            return pickle.load(f)


def sort_tweetset_chronologically(tweet_list:list) -> None:
    " Sorts a list of tweepy tweets by 'tweet.created_at field'"
    tweet_list.sort(key=lambda tweet: tweet.created_at, reverse=False)


def merge_datasets_by_directory(input_dir:str, output_dir:str) -> None:
    """ Takes all datasets (multiple pickle file containing a list
        of tweepy tweets each) in 'input_dir', merges them into
        one single compressed pickle file containing one list of
        tweepy tweets. Output goes to 'output_dir', naturally.
    """
     # // Get all pickled lists.
    cache = []
    for name in get_filenames_in_dir(path=input_dir):
        # // Attempt to fetch content.
        content = get_file_content(f"{input_dir}{name}")
        if content:
            cache.extend(content)
    # // Sort and write new file
    if cache:
        sort_tweetset_chronologically(cache)
        common.save_data(
            content=cache,
            out_dir=output_dir,
            compressed=True
        )
    else:
        print("Did not merge dataset, check input and output files.")


def split_dataset_by_obj_count(divider:int, filename:str, out_dir:str) -> None:
    """ Uses 'filename' to unpickle (can be compressed) a dataset containing
        a list of tweepy tweets. This data is then split it into N equal 
        (approximately) parts, where N is specified by the 'divider' param.
        These chunks will then be saved to the destination directory specified
        by 'out_dir'.
    """
    # // Get content and check it.
    cache_continious = get_file_content(filename=filename)
    if not cache_continious:
        print(f"Found no content in '{filename}'")
        return
    # // Sort for logical filaname ordering.
    sort_tweetset_chronologically(cache_continious)
    # // Temporary holder of content
    cache_chunked = []
    chunk_size = int(len(cache_continious) / divider)
    print(f"splitting: len: {len(cache_continious)}")#@
    while cache_continious:
        # // Transfer items between lists
        last_item = cache_continious.pop()
        cache_chunked.append(last_item)
        # // Save on chunksize
        if len(cache_chunked) >= chunk_size:
            # // Grab remainders, if any
            if len(cache_continious) < chunk_size:
                cache_chunked.extend(cache_continious)
            # // Save
            sort_tweetset_chronologically(cache_chunked)
            common.save_data(
                content=cache_chunked,
                out_dir=out_dir,
                compressed=True
            )
            cache_chunked.clear()
